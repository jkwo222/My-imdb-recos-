name: nightly

on:
  schedule:
    - cron: '30 7 * * *'   # 07:30 UTC (3:30am ET)
  workflow_dispatch:

permissions:
  contents: read
  issues: write

jobs:
  build:
    runs-on: ubuntu-24.04
    env:
      # Region / discovery
      REGION: US
      ORIGINAL_LANGS: '["en"]'
      SUBS_INCLUDE: apple_tv_plus,netflix,max,paramount_plus,disney_plus,peacock,hulu
      DISCOVER_PAGES: "12"
      DISCOVER_PAGING_MODE: random
      DISCOVER_PAGE_MAX: "200"

      # TMDB auth (either works)
      TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
      TMDB_BEARER:  ${{ secrets.TMDB_ACCESS_TOKEN }}

      # IMDb (optional)
      IMDB_USER_ID: ${{ secrets.IMDB_USER_ID }}
      IMDB_PUBLIC_MAX_PAGES: "10"

      # Enrichment knobs
      ENRICH_PROVIDERS_TOP_N: "220"
      ENRICH_SCORING_TOP_N: "220"

      # search_multi() fallback tuning (optional)
      SEARCH_MULTI_ON_EMPTY_DETAILS: "true"
      SEARCH_MULTI_ON_MISSING_ID: "true"
      SEARCH_MULTI_TITLE_SIM_TH: "0.62"
      SEARCH_MULTI_YEAR_WEIGHT: "0.35"
      SEARCH_MULTI_TYPE_BONUS: "0.25"

      # Profile model knobs
      AFFINITY_K: "5"
      DECAY_HALF_LIFE_DAYS: "270"

      # Scoring blend knobs
      AUDIENCE_PRIOR_LAMBDA: "0.3"
      PROVIDER_PREF_LAMBDA: "0.5"

      # Anime / Kids penalties
      PENALIZE_KIDS: "true"
      PENALIZE_ANIME: "true"
      KIDS_CARTOON_PENALTY: "25"
      ANIME_PENALTY: "20"

      # Email layout knobs
      EMAIL_TOP_MOVIES: "10"
      EMAIL_TOP_TV: "10"
      EMAIL_SCORE_MIN: "30"
      EMAIL_BACKFILL: "true"
      EMAIL_BACKFILL_MIN: "20"
      EMAIL_BACKFILL_MOVIE_MIN: "20"
      EMAIL_BACKFILL_TV_MIN: "20"
      EMAIL_BACKFILL_ALLOW_ROTATE: "true"
      EMAIL_BACKFILL_FETCH_PROVIDERS: "true"
      EMAIL_EARLY_FETCH_PROVIDERS: "true"
      EMAIL_INCLUDE_TELEMETRY: "true"
      EMAIL_INCLUDE_NEW_MOVIE_LABEL: "true"
      EMAIL_INCLUDE_NEW_SERIES_LABEL: "true"
      EMAIL_INCLUDE_NEW_SEASON_LABEL: "true"
      EMAIL_NETWORK_FALLBACK: "true"

      # Rotation cooldown
      ROTATION_ENABLE: "true"
      ROTATION_COOLDOWN_DAYS: "5"

      # Optional @mention for notifications
      NOTIFY_USER: ${{ vars.NOTIFY_USER }}

      # Cache version for pool (bump to invalidate)
      POOL_CACHE_VERSION: v1
      POOL_MAX_ITEMS: "20000"

      # ---- Rate-limit controls for per-title feedback comments ----
      FEEDBACK_MAX_COMMENTS: "10"   # cap how many per-title comments to upsert per run
      FEEDBACK_RETRIES: "3"         # retries on secondary-rate-limit
      FEEDBACK_SLEEP_MS: "900"      # base backoff (ms); doubled on each retry

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      # Restore persistent caches so the pool truly grows
      - name: Restore pool/tmdb caches
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/cache/pool
            data/cache/tmdb
          key: pool-${{ runner.os }}-${{ env.REGION }}-${{ env.POOL_CACHE_VERSION }}-${{ github.run_id }}
          restore-keys: |
            pool-${{ runner.os }}-${{ env.REGION }}-${{ env.POOL_CACHE_VERSION }}-

      - name: Run engine (capture log safely)
        shell: bash
        run: |
          set -o pipefail
          mkdir -p data/out data/cache data/out/latest
          python -m engine.runner | tee data/out/latest/runner.log

      # Build digest using summarize module (exports feedback targets too)
      - name: Build digest summary (Top Movies & Top Shows)
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          from engine.summarize import write_email_markdown
          run_dir = Path("data/out/latest")
          env = {
            "REGION": os.getenv("REGION","US"),
            "SUBS_INCLUDE": [s.strip() for s in (os.getenv("SUBS_INCLUDE","").split(",") if os.getenv("SUBS_INCLUDE") else [])],
          }
          out = write_email_markdown(
            run_dir=run_dir,
            ranked_items_path=run_dir / "items.enriched.json",
            env=env,
          )
          print("summary:", out, "exists:", out.exists())
          PY

      - name: Make debug bundle
        run: |
          chmod +x ./.github/scripts/make_debug_bundle.sh || true
          bash ./.github/scripts/make_debug_bundle.sh

      # Create a clean repo snapshot artifact so uploads never fail
      - name: Create repo snapshot (tracked files at HEAD)
        run: |
          set -e
          rm -f repo-snapshot.zip
          git archive -o repo-snapshot.zip HEAD
          ls -lh repo-snapshot.zip

      - name: Post/update Daily Recommendations issue + per-title feedback comments (rate-limit aware)
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const title = 'Daily Recommendations';

            const MAX = parseInt(process.env.FEEDBACK_MAX_COMMENTS || '10', 10);
            const RETRIES = parseInt(process.env.FEEDBACK_RETRIES || '3', 10);
            const SLEEP_MS = parseInt(process.env.FEEDBACK_SLEEP_MS || '900', 10);

            const sleep = (ms) => new Promise(r => setTimeout(r, ms));
            async function tryCall(fn, args, desc) {
              let attempt = 0;
              while (true) {
                try {
                  return await fn(args);
                } catch (e) {
                  const msg = (e?.response?.data?.message) || e.message || String(e);
                  // Secondary rate limit handling
                  if (e.status === 403 && /secondary rate limit/i.test(msg) && attempt < RETRIES) {
                    const ra = parseInt(e?.response?.headers?.['retry-after'] || '0', 10);
                    const backoff = (ra > 0 ? ra * 1000 : SLEEP_MS * Math.pow(2, attempt));
                    core.warning(`Rate limited while ${desc}. Sleeping ${Math.round(backoff)}ms (attempt ${attempt+1}/${RETRIES})`);
                    await sleep(backoff);
                    attempt++;
                    continue;
                  }
                  throw e;
                }
              }
            }

            // 1) Load the summary body
            let body = '';
            try { body = fs.readFileSync('data/out/latest/summary.md', 'utf8'); }
            catch { body = 'No summary.md found. Check debug bundle.'; }

            // 2) Ensure the issue exists (label: daily-recos)
            const { data: issues } = await github.rest.issues.listForRepo({
              owner, repo, state: 'open', labels: 'daily-recos', per_page: 100
            });
            let issue = issues.find(i => i.title === title);
            if (!issue) {
              issue = (await tryCall(
                github.rest.issues.create,
                { owner, repo, title, body, labels: ['daily-recos'] },
                'creating issue'
              )).data;
            } else {
              await tryCall(
                github.rest.issues.update,
                { owner, repo, issue_number: issue.number, body },
                'updating issue body'
              );
            }

            // 3) Load feedback targets exported by summarize.py
            let targets = [];
            try {
              const raw = fs.readFileSync('data/out/latest/exports/feedback_targets.json', 'utf8');
              const parsed = JSON.parse(raw);
              targets = parsed.targets || [];
            } catch (e) {
              core.warning('No feedback_targets.json found; skipping per-title comments.');
              targets = [];
            }

            // 4) Map existing comments by marker key <!-- reco:KEY -->
            const markerRe = /<!--\s*reco:([^\s>]+)\s*-->/;
            const allComments = await github.paginate(
              github.rest.issues.listComments,
              { owner, repo, issue_number: issue.number, per_page: 100 }
            );
            const existingByKey = new Map();
            for (const c of allComments) {
              const m = markerRe.exec(c.body || "");
              if (m) existingByKey.set(m[1], c);
            }

            // 5) Prepare only new/changed comments; cap by MAX to avoid burst
            const upserts = [];
            for (const t of targets) {
              const key = t.key;
              const desired = t.comment_md || `Feedback for ${t.title}${t.year ? ' ('+t.year+')' : ''}\n\n<!-- reco:${key} -->`;
              const ex = existingByKey.get(key);
              if (!ex || (ex.body || '') !== desired) {
                upserts.push({ key, desired, existing: ex });
              }
            }
            const slice = upserts.slice(0, Math.max(0, MAX));

            // 6) Upsert with backoff + small pacing pause between calls
            for (const u of slice) {
              if (!u.existing) {
                await tryCall(
                  github.rest.issues.createComment,
                  { owner, repo, issue_number: issue.number, body: u.desired },
                  `creating comment for ${u.key}`
                );
              } else {
                await tryCall(
                  github.rest.issues.updateComment,
                  { owner, repo, comment_id: u.existing.id, body: u.desired },
                  `updating comment for ${u.key}`
                );
              }
              await sleep(300); // tiny pacing pause between calls
            }

            // 7) Optional: mention a user (do this last to reduce content ops)
            const mention = process.env.NOTIFY_USER ? ` cc @${process.env.NOTIFY_USER}` : '';
            if (mention) {
              await tryCall(
                github.rest.issues.createComment,
                { owner, repo, issue_number: issue.number, body: `Daily digest posted.${mention}` },
                'creating notify comment'
              );
            }

      - name: Upload debug bundle
        uses: actions/upload-artifact@v4
        with:
          name: debug-data
          path: debug-data.zip
          if-no-files-found: warn

      - name: Upload repo snapshot
        uses: actions/upload-artifact@v4
        with:
          name: repo-snapshot
          path: repo-snapshot.zip
          if-no-files-found: warn

      - name: Upload latest outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: latest-run
          if-no-files-found: ignore
          path: |
            data/out/latest/runner.log
            data/out/latest/assistant_feed.json
            data/out/latest/items.discovered.json
            data/out/latest/items.enriched.json
            data/out/latest/summary.md
            data/out/latest/diag.json
            data/out/latest/exports/selection_breakdown.json
            data/out/latest/exports/feedback_targets.json
            data/out/latest/exports/user_model.json

      # Save caches so next run restores an ever-growing pool
      - name: Save pool/tmdb caches
        if: always()
        uses: actions/cache/save@v4
        with:
          path: |
            data/cache/pool
            data/cache/tmdb
          key: pool-${{ runner.os }}-${{ env.REGION }}-${{ env.POOL_CACHE_VERSION }}-${{ github.run_id }}